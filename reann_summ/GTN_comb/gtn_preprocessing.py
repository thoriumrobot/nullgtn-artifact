# -*- coding: utf-8 -*-
"""GTN_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fb5XBg4qX2J-jc6grQeXnPhXOnGh9d94
"""

import json
import numpy as np
from scipy.sparse import csr_matrix
import os
import subprocess
import sys

directory = '/home/k/ks225/prune/prune_refac2/'
nodeList=["MethodDeclaration", "Parameter", "FieldDeclaration", "ArrayType", "ClassOrInterfaceType", "VariableDeclarationExpr"]

fname=os.path.join(directory, "output_prune.json")

print(fname)

json_data = []

# Load JSON data from file
with open(fname, "r") as file:
  json_data = json.load(file)

json_data=list(np.random.choice(json_data,30 if len(json_data)>30 else len(json_data),replace=False))

print(len(json_data))
  
if len(json_data)<2:
  print("no data")
  sys.exit()

type(json_data)

types=set()

for graph_json in json_data:
  nodes=graph_json['nodes']
  for node in nodes:
    types.add(node['type'])

print(len(types))

print(types)

type(json_data[0]['adjacencyList'])

json_data[0]

#type(json_data[0]['adjacencyList'][str(json_data[0]['nodes'][350]['id'])])

types=list(types)
#types.index("Name")

"""# Edges"""

nnode=0
nmax=0

for graph_json in json_data:
  nodes=graph_json['nodes']
  for node in nodes:
    nnode+=1
    nmax=max(nmax, node['id'])

print('nnode = ',nnode)
print('nmax = ',nmax)

nterm=0

for graph_json in json_data:
  nterm+=len(graph_json['nameList'])

print('nterm = ',nterm)

nnum=nnode+nterm

print("nnum = ",nnum)

A_n=np.zeros((nnum, nnum))

gnode=0
csum=0

for graph_json in json_data:
  nodes=graph_json['nodes']
  alist=graph_json['adjacencyList']
  for node in nodes:
    if str(node['id']) in alist:
      for neighbor in alist[str(node['id'])]:
        A_n[gnode,neighbor+csum]=1
    gnode+=1
  csum+=len(nodes)

A_n=csr_matrix(A_n)

A_t=np.zeros((nnum, nnum))

tsum=0
csum=0

for graph_json in json_data:
  nodes=len(graph_json['nodes'])
  nlist=graph_json['nameList']
  tnode=0
  for tkey in nlist:
    for node in nlist[tkey]:
      A_t[csum+node,nnode+tsum+tnode]=1
    tnode+=1
  csum+=nodes
  tsum+=tnode

A_t=csr_matrix(A_t)

import pickle

edges=[A_n,A_n.transpose(),A_t,A_t.transpose()]
with open('/home/k/ks225/ablation/data2/Null/edges.pkl', 'wb') as f:
  pickle.dump(edges, f)

"""# Node Features"""

#last feature is nullable
null_feat=np.zeros((nnum,2))
type_feat=np.zeros((nnum,len(types)+1))

gnode=0

snowflakes=[]

for graph_json in json_data:
  nodes=graph_json['nodes']
  for node in nodes:
    if node['type'] in nodeList:
      snowflakes.append(gnode)
    if node['nullable']==0:
      null_feat[gnode,1]=1
    else:
      null_feat[gnode,0]=1
    type_feat[gnode,types.index(node['type'])]=1
    gnode+=1

print("snowflakes =",len(snowflakes))

for node_i in range(gnode,nnum):
  null_feat[node_i,0]=1
  type_feat[node_i,len(types)]=1

#node_feature=np.concatenate((null_feat,type_feat), axis=1)
with open('/home/k/ks225/ablation/data2/Null/node_features.pkl', 'wb') as f:
  pickle.dump(type_feat, f)

"""# Label"""

nullnodes=0

for graph_json in json_data:
  nodes=graph_json['nodes']
  for node in nodes:
    if node['nullable']==1:
      nullnodes+=1

nullnodes

# Find the row indices where the value in the specified column is 1
row_indices = [x for x in np.where(null_feat[:, 1] == 1)[0] if x in snowflakes]
#row_indices = np.where(null_feat[:, 1] == 1)[0]

print(len(row_indices))

if not row_indices:
  sys.exit()

train_size=int(0.6*len(row_indices))
other_size=int(0.2*len(row_indices))
print('Train = ', train_size)
print('Valid/Test = ', other_size)

nonnullable_idx = [x for x in np.where(null_feat[:, 0] == 1)[0] if x < nnode and x in snowflakes]
#nonnullable_idx = [x for x in np.where(null_feat[:, 0] == 1)[0] if x < nnode]

print(len(nonnullable_idx))

if not nonnullable_idx:
  sys.exit()

train_null=list(np.random.choice(row_indices,train_size,replace=False))
train_nonnull=list(np.random.choice(nonnullable_idx,train_size if train_size<len(nonnullable_idx) else int(0.6*len(nonnullable_idx)),replace=False))

train_idx=np.array(train_null + train_nonnull)
#print(train_idx)
if len(train_idx)<1:
  print("train_idx empty")
  sys.exit()
train_target=null_feat[train_idx,1]

train_label = np.vstack((train_idx,train_target)).transpose()

remaining_null=[item for item in row_indices if item not in train_null]
remaining_nonnull=[item for item in nonnullable_idx if item not in train_nonnull]

valid_null=list(np.random.choice(remaining_null,other_size,replace=False))
valid_nonnull=list(np.random.choice(remaining_nonnull,other_size if other_size<len(remaining_nonnull) else int(0.2*len(nonnullable_idx)),replace=False))

valid_idx=np.array(valid_null + valid_nonnull)
if len(valid_idx)<1:
  print("valid_idx empty")
  sys.exit()
valid_target=null_feat[valid_idx,1]

valid_label = np.vstack((valid_idx,valid_target)).transpose()

remaining_null=[item for item in remaining_null if item not in valid_null]
remaining_nonnull=[item for item in remaining_nonnull if item not in valid_nonnull]

test_idx=np.array(remaining_null+remaining_nonnull)
if len(test_idx)<1:
  print("test_idx empty")
  sys.exit()
test_target=null_feat[test_idx,1]

test_label = np.vstack((test_idx,test_target)).transpose()

#Creates a list containing the train, validation, and test labels.
labels = [train_label,valid_label,test_label]

with open('/home/k/ks225/ablation/data2/Null/labels.pkl', 'wb') as f:
  pickle.dump(labels, f)

i_l='3'
i_lr='0.04'
i_nlw='-2'
i_k='3'

subprocess.call(['python', 'main.py', '--dataset', 'Null', '--model', 'FastGTN', '--num_layers', i_l, '--epoch', '200', '--lr', i_lr, '--channel_agg', 'mean', '--num_channels', '2', '--non_local_weight', i_nlw, '--K', i_k, '--non_local'])

